{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈 임포트하기\n",
    "import pandas as pd\n",
    "from pandas import DataFrame  as df\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./DATA/Seoul_2015.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-01 0:27</td>\n",
       "      <td>수원에 있는 성대 대학원까지 서울대 졸업한 애들이 오는 이유가 뭐냐 안 불편하나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-01 0:51</td>\n",
       "      <td>공대생들이거봐라 니들 구글코리아 신입으로 입사하고 막 그러냐구글코리아 들어간사람있냐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-01 1:47</td>\n",
       "      <td>어제오늘 일해서 백만원 벌었다  기쁘다 한학기 등록금  벌음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-01 2:38</td>\n",
       "      <td>사기내 다 쓸어버리네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-01 6:10</td>\n",
       "      <td>잠때문에 죽겄다 학기중에도 밤에 잠못자고 낮에 졸린것 때문에 고생했는데방학하니까 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>2015-09-27 12:27</td>\n",
       "      <td>신환회비 너무 아까운 부분 새내기들 철저하게 밥 안사주기로 결심</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>2015-09-27 12:27</td>\n",
       "      <td>나랑 노래 배틀 뜰 새끼 구한다 코인노래방에서 붙자덤벼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7719</th>\n",
       "      <td>2015-09-27 12:27</td>\n",
       "      <td>지금 일반인 개방 아예 안되어있는 도서관 신중도밖에없 관정도 맘만먹으면 타대생 올 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>2015-09-27 12:27</td>\n",
       "      <td>신입생 지원품 패키지이게 머에얌 궁금궁금</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>2015-09-27 12:27</td>\n",
       "      <td>항상 참인 명제 치킨은 항상 옳다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time                                              total\n",
       "0      2015-03-01 0:27       수원에 있는 성대 대학원까지 서울대 졸업한 애들이 오는 이유가 뭐냐 안 불편하나\n",
       "1      2015-03-01 0:51  공대생들이거봐라 니들 구글코리아 신입으로 입사하고 막 그러냐구글코리아 들어간사람있냐...\n",
       "2      2015-03-01 1:47                  어제오늘 일해서 백만원 벌었다  기쁘다 한학기 등록금  벌음\n",
       "3      2015-03-01 2:38                                        사기내 다 쓸어버리네\n",
       "4      2015-03-01 6:10  잠때문에 죽겄다 학기중에도 밤에 잠못자고 낮에 졸린것 때문에 고생했는데방학하니까 자...\n",
       "...                ...                                                ...\n",
       "7717  2015-09-27 12:27                신환회비 너무 아까운 부분 새내기들 철저하게 밥 안사주기로 결심\n",
       "7718  2015-09-27 12:27                     나랑 노래 배틀 뜰 새끼 구한다 코인노래방에서 붙자덤벼\n",
       "7719  2015-09-27 12:27  지금 일반인 개방 아예 안되어있는 도서관 신중도밖에없 관정도 맘만먹으면 타대생 올 ...\n",
       "7720  2015-09-27 12:27                             신입생 지원품 패키지이게 머에얌 궁금궁금\n",
       "7721  2015-09-27 12:27                                 항상 참인 명제 치킨은 항상 옳다\n",
       "\n",
       "[7722 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 입력 받은 단어와 기간으로 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['time'] = pd.to_datetime(Data[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일자 별 문서 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-01 00:27:00</td>\n",
       "      <td>수원에 있는 성대 대학원까지 서울대 졸업한 애들이 오는 이유가 뭐냐 안 불편하나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-01 00:51:00</td>\n",
       "      <td>공대생들이거봐라 니들 구글코리아 신입으로 입사하고 막 그러냐구글코리아 들어간사람있냐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-01 01:47:00</td>\n",
       "      <td>어제오늘 일해서 백만원 벌었다  기쁘다 한학기 등록금  벌음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-01 02:38:00</td>\n",
       "      <td>사기내 다 쓸어버리네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-01 06:10:00</td>\n",
       "      <td>잠때문에 죽겄다 학기중에도 밤에 잠못자고 낮에 졸린것 때문에 고생했는데방학하니까 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>2015-09-27 12:27:00</td>\n",
       "      <td>신환회비 너무 아까운 부분 새내기들 철저하게 밥 안사주기로 결심</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>2015-09-27 12:27:00</td>\n",
       "      <td>나랑 노래 배틀 뜰 새끼 구한다 코인노래방에서 붙자덤벼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7719</th>\n",
       "      <td>2015-09-27 12:27:00</td>\n",
       "      <td>지금 일반인 개방 아예 안되어있는 도서관 신중도밖에없 관정도 맘만먹으면 타대생 올 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>2015-09-27 12:27:00</td>\n",
       "      <td>신입생 지원품 패키지이게 머에얌 궁금궁금</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7721</th>\n",
       "      <td>2015-09-27 12:27:00</td>\n",
       "      <td>항상 참인 명제 치킨은 항상 옳다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time                                              total\n",
       "0    2015-03-01 00:27:00       수원에 있는 성대 대학원까지 서울대 졸업한 애들이 오는 이유가 뭐냐 안 불편하나\n",
       "1    2015-03-01 00:51:00  공대생들이거봐라 니들 구글코리아 신입으로 입사하고 막 그러냐구글코리아 들어간사람있냐...\n",
       "2    2015-03-01 01:47:00                  어제오늘 일해서 백만원 벌었다  기쁘다 한학기 등록금  벌음\n",
       "3    2015-03-01 02:38:00                                        사기내 다 쓸어버리네\n",
       "4    2015-03-01 06:10:00  잠때문에 죽겄다 학기중에도 밤에 잠못자고 낮에 졸린것 때문에 고생했는데방학하니까 자...\n",
       "...                  ...                                                ...\n",
       "7717 2015-09-27 12:27:00                신환회비 너무 아까운 부분 새내기들 철저하게 밥 안사주기로 결심\n",
       "7718 2015-09-27 12:27:00                     나랑 노래 배틀 뜰 새끼 구한다 코인노래방에서 붙자덤벼\n",
       "7719 2015-09-27 12:27:00  지금 일반인 개방 아예 안되어있는 도서관 신중도밖에없 관정도 맘만먹으면 타대생 올 ...\n",
       "7720 2015-09-27 12:27:00                             신입생 지원품 패키지이게 머에얌 궁금궁금\n",
       "7721 2015-09-27 12:27:00                                 항상 참인 명제 치킨은 항상 옳다\n",
       "\n",
       "[7722 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_count = {}\n",
    "for item in Data['time']:\n",
    "    element_count.setdefault(item, 0)\n",
    "    element_count[item] += 1\n",
    "doc_count = pd.DataFrame.from_dict(element_count, orient='index', columns=[\"doc_count\"])\n",
    "\n",
    "Data.total = Data.total.astype(str)\n",
    "\n",
    "clean_Data = Data\n",
    "clean_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yss\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\missing.py:48: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask = arr == x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#데이터 프레임의 'text' 열의 값 중 keyword1이나 keyword 2가 포함된 행은 삭제\n",
    "#clean_Data = Data[~Data['text'].str.contains('keyword1|keyword2')]\n",
    "\n",
    "#text와 timestamp 열을 기준으로 중복된 데이터를 삭제, inplace : 데이터 프레임을 변경할지 선택(원본을 대체)\n",
    "clean_Data.drop_duplicates(subset=['total','time'], inplace=True)\n",
    "\n",
    "#한글이 아니면 빈 문자열로 바꾸기\n",
    "clean_Data['total'] = clean_Data['total'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]',' ',regex=True)\n",
    "\n",
    "#빈 문자열 NAN 값으로 바꾸기\n",
    "clean_Data = clean_Data.replace({'': np.nan})\n",
    "clean_Data = clean_Data.replace(r'^\\s*$', None, regex=True)\n",
    "\n",
    "#NAN 이 있는 행은 삭제\n",
    "clean_Data.dropna(how='any', inplace=True)\n",
    "\n",
    "#인덱스 차곡차곡\n",
    "clean_Data = clean_Data.reset_index(drop=True)\n",
    "\n",
    "print(clean_Data.isnull().values.any()) \n",
    "\n",
    "\n",
    "#텍스트 데이터를 리스트로 변환\n",
    "Data_list = clean_Data.total.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#리스트를 요소별로(트윗 하나) 가져와서 명사만 추출한 후 리스트로 저장\n",
    "data_word=[]\n",
    "for i in range(len(Data_list)):\n",
    "    try:\n",
    "        data_word.append(okt.nouns(Data_list[i]))\n",
    "    except Exception as e:\n",
    "        continue\n",
    "Data['clean'] = data_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "id2word=corpora.Dictionary(data_word)\n",
    "id2word.filter_extremes(no_below = 20)\n",
    "texts = data_word\n",
    "corpus=[id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# os.environ['MALLET_HOME'] = 'C:/Users/YSS/Downloads/mallet-2.0.8'\n",
    "\n",
    "mallet_path = 'D:/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)\n",
    "\n",
    "\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=data_word, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=4, limit=21, step=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=21; start=4; step=2;\n",
    "x = range(start, limit, step)\n",
    "topic_num = 0\n",
    "count = 0\n",
    "max_coherence = 0\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", cv)\n",
    "    coherence = cv\n",
    "    if coherence >= max_coherence:\n",
    "        max_coherence = coherence\n",
    "        topic_num = m\n",
    "        model_list_num = count   \n",
    "    count = count+1\n",
    "\n",
    "        \n",
    "# Select the model and print the topics\n",
    "optimal_model = model_list[model_list_num]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "#print(optimal_model.print_topics(num_words=10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    #ldamodel[corpus]: lda_model에 corpus를 넣어 각 토픽 당 확률을 알 수 있음\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num,topn=10)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    print(type(sent_topics_df))\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    #contents = pd.Series(texts)\n",
    "    #sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, Data['total'],Data['time'],Data['clean']], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=Data_list)\n",
    "\n",
    "# Format\n",
    "df_topic_tweet = df_topic_sents_keywords.reset_index()\n",
    "df_topic_tweet.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'total','Time','Clean']\n",
    "\n",
    "# Show각 문서에 대한 토픽\n",
    "#df_dominant_topic=df_dominant_topic.sort_values(by=['Dominant_Topic'])\n",
    "#df_topic_tweet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "topic_counts.sort_index(inplace=True)\n",
    "\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_contribution\n",
    "\n",
    "lda_inform = pd.concat([sent_topics_sorteddf_mallet, topic_counts, topic_contribution], axis=1)\n",
    "lda_inform.columns=[\"Topic_Num\", \"Topic_Perc_Contrib\", \"Keywords\", \"total\", \"time\",\"Clean\",\"Num_Documents\", \"Perc_Documents\"]\n",
    "lda_inform = lda_inform[[\"Topic_Num\",\"Keywords\",\"Num_Documents\",\"Perc_Documents\"]]\n",
    "lda_inform\n",
    "#lda_inform.Topic_Num = lda_inform.Topic_Num.astype(int)\n",
    "lda_inform['Topic_Num'] =lda_inform['Topic_Num'] +1\n",
    "lda_inform.Topic_Num = lda_inform.Topic_Num.astype(str)\n",
    "lda_inform['Topic_Num'] =lda_inform['Topic_Num'].str.split('.').str[0]\n",
    "df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'] +1\n",
    "df_topic_tweet.Dominant_Topic = df_topic_tweet.Dominant_Topic.astype(str)\n",
    "df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'].str.split('.').str[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 별 트윗 퍼센트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lda_inform.to_csv (\"./Result/lda_inform.csv\", index = None)\n",
    "lda_inform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_topic_tweet.to_csv (\"./Result/df_topic_tweet.csv\", index = None)\n",
    "df_topic_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 별 트윗 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토픽별 트윗 저장\n",
    "for i in range(1,topic_num+1):\n",
    "    globals()['df_{}'.format(i)]=df_topic_tweet.loc[df_topic_tweet.Dominant_Topic==str(i)]\n",
    "    globals()['df_{}'.format(i)].sort_values('Topic_Perc_Contrib',ascending=False,inplace = True)\n",
    "    globals()['df_{}'.format(i)].to_csv (\"./Result/topic(\"+str(i)+\")_tweet.csv\", index = None)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1~df_?? 토픽 수 까지\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 별 word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,topic_num+1):\n",
    "    #data_list = globals()['df_{}'.format(i)].Text.values.tolist()\n",
    "    long_string = sum(globals()['df_{}'.format(i)].Clean.values,[])\n",
    "    str(long_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #data_word=[x for x in data_word if not x.isdigit()]\n",
    "    \n",
    "    freq=pd.Series(long_string).value_counts().head(50)\n",
    "    freq=dict(freq)\n",
    "    # Create a WordCloud object\n",
    "    freq=dict(freq)\n",
    "    \n",
    "    wordcloud = WordCloud(font_path=\"./Font/BMHANNA_11yrs_ttf.ttf\",\n",
    "             relative_scaling = 0.2,\n",
    "             background_color = 'white',\n",
    "            ).generate_from_frequencies(freq)\n",
    "    \n",
    "    # Visualize the word cloud\n",
    "    wordcloud.to_image()\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(\"./Result/topic(\"+str(i)+\")wordcloud.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 별 긍부정 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1,topic_num+1):\n",
    "    globals()['df_{}_pn'.format(i)]=globals()['df_{}'.format(i)].label.value_counts(normalize=True) * 100\n",
    "    globals()['df_{}_pn'.format(i)]\n",
    "  #  globals()['df_{}_pn'.format(i)].to_csv (\"./Result/topic_tweet(topic\"+str(i)+\")posneg.csv\", index = None)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1_pn~df_?? 토픽 수 까지\n",
    "df_1_pn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 별 긍정 부정 트윗 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, topic_num+1):\n",
    "    globals()['df_{}_positive'.format(i)]=globals()['df_{}'.format(i)][globals()['df_{}'.format(i)]['label'] == 1]\n",
    "    globals()['df_{}_negative'.format(i)]=globals()['df_{}'.format(i)][globals()['df_{}'.format(i)]['label'] == 0]\n",
    "    globals()['df_{}_positive'.format(i)].to_csv (\"./Result/topic(\"+str(i)+\")_tweet_positive.csv\", index = None)\n",
    "    globals()['df_{}_negative'.format(i)].to_csv (\"./Result/topic(\"+str(i)+\")_tweet_negative.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= pd.read_csv('./KOREA_Tweet_Data.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
